import streamlit as st
from langchain import hub
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
import bs4

# Streamlit í˜ì´ì§€ ì„¤ì •
st.title("ğŸ¤–  ChatBot with RAG")

# ë©”ì‹œì§€ ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini")

# RAG ì†ŒìŠ¤ URL ì„¤ì •
source_url = "https://spartacodingclub.kr/blog/all-in-challenge_winner"

# ì›¹ ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™” (SoupStrainerë¡œ ì›í•˜ëŠ” ë‚´ìš©ë§Œ ë¡œë“œ)
loader = WebBaseLoader(
    web_paths=(source_url,),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("css-8lvslw")  # í•„ìš”í•œ HTML í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ ìˆ˜ì •
        )
    ),
)

# ì›¹ ë¬¸ì„œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• 
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings(),
    persist_directory=".chroma_data"  # ë°ì´í„° ì €ì¥ ê²½ë¡œ ì§€ì •
)
retriever = vectorstore.as_retriever()

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¬ëŠ” ì…ë ¥ ì°½
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        retrieved_docs = retriever.invoke(prompt)

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # RAG í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        prompt_template = hub.pull("rlm/rag-prompt")
        user_prompt = prompt_template.invoke({"context": format_docs(retrieved_docs), "question": prompt})

        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        response = llm.invoke(user_prompt)

    # ë‹µë³€ ì¶œë ¥ ë° ì„¸ì…˜ì— ì €ì¥
    with st.chat_message("assistant"):
        st.markdown(response.content)
    st.session_state.messages.append({"role": "assistant", "content": response.content})
