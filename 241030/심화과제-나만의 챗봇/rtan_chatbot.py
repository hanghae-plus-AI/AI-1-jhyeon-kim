import streamlit as st
from langchain import hub
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage


# Streamlit í˜ì´ì§€ ì„¤ì •
st.title("íŒ€ìŠ¤íŒŒë¥´íƒ€ ì˜¨ë³´ë”© ì•ˆë‚´ë´‡ğŸ¥ ")

# ë©”ì‹œì§€ ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o")

pdf_paths = ["./teamsparta/vacation.pdf", "./teamsparta/Codebook.pdf",
             "./teamsparta/one-goal.pdf", "./teamsparta/Tuition Assistance.pdf",
             "./teamsparta/routine.pdf", "./teamsparta/Counseling.pdf",
             "./teamsparta/communication_culture.pdf", "./teamsparta/values.pdf",
             "./teamsparta/monthly_goal.pdf",
             ]  # ì‚¬ìš©í•  PDF íŒŒì¼ ê²½ë¡œ ì¶”ê°€
docs = []
for path in pdf_paths:
    loader = PyPDFLoader(path)
    docs.extend(loader.load())

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings(),
    persist_directory=".rtan_chatbot_chroma_data"
)
retriever = vectorstore.as_retriever(search_kwargs={"k":5})

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ì„ ê¸°ë‹¤ë¦¬ëŠ” ì…ë ¥ ì°½
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        retrieved_docs = retriever.invoke(prompt)

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # ChatPromptTemplate ìƒì„±
        prompt_template = ChatPromptTemplate.from_messages([
            HumanMessage(content="ë„ˆëŠ” íŒ€ìŠ¤íŒŒë¥´íƒ€ë¼ëŠ” íšŒì‚¬ì˜ ì˜¨ë³´ë”©(ì‹ ê·œ ì…ì‚¬ìë“¤ì—ê²Œ íšŒì‚¬ ë¬¸í™”ë‚˜ ê·œì •ì„ ì•Œë ¤ì£¼ëŠ” ê²ƒ)ì„ ë„ì™€ì£¼
ëŠ” ì±—ë´‡ì´ì•¼. ì²¨ë¶€í•œ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ íŒ€ìŠ¤íŒŒë¥´íƒ€ì— ëŒ€í•´ ì‚¬ìš©ìê°€ ë¬»ëŠ” ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜."
                                 "íŒ€ìŠ¤íŒŒë¥´íƒ€ ë¬¸í™”ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì´ê±°ë‚˜ ì˜ ëª¨ë¥´ëŠ” ì •ë³´ë¼ë©´ ë‹µí•˜ì§€ ë§ì•„ì¤˜. ë‹¤ë§Œ ë‹µí•  ìˆ˜ ìˆëŠ” >ë‚´ìš©ì— ëŒ€í•´ì„œëŠ” ê°€ëŠ¥í•œ ê°€ì¥ ìì„¸í•˜ê²Œ ë‹µë³€í•´ì¤˜.\n"
                                 f"Question: {prompt}\nContext: {format_docs(retrieved_docs)}")
        ])

        # ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ ìƒì„±
        user_prompt = prompt_template.format()


        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        response = llm.invoke(user_prompt)



    # ë‹µë³€ ì¶œë ¥ ë° ì„¸ì…˜ì— ì €ì¥
    with st.chat_message("assistant"):
        st.markdown(response.content)
    st.session_state.messages.append({"role": "assistant", "content": response.content})
