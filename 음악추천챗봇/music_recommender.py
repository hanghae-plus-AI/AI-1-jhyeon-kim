import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import streamlit as st
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage

# ê°ì • ë¶„ë¥˜ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
model = AutoModelForSequenceClassification.from_pretrained("jeonghyeon97/koBERT-Senti5", trust_remote_code=True)

# ê°ì • ë ˆì´ë¸”ê³¼ ì´ëª¨ì§€ ë§¤í•‘
emotion_labels = {
    0: ("Angry", "ğŸ˜¡"),
    1: ("Fear", "ğŸ˜¨"),
    2: ("Happy", "ğŸ˜Š"),
    3: ("Tender", "ğŸ¥°"),
    4: ("Sad", "ğŸ˜¢")
}

# Streamlit í˜ì´ì§€ ì„¤ì •
st.title("ê°ì • ê¸°ë°˜ ì¶”ì²œ ìŒì•… ì±—ë´‡ ğŸ¶")
st.write("ì¼ê¸°ë‚˜ ì˜¤ëŠ˜ì˜ ê°ì •ì„ ì…ë ¥í•˜ë©´, ì–´ìš¸ë¦¬ëŠ” ìŒì•…ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.")

# ë©”ì‹œì§€ ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# PDF ë¬¸ì„œ ë¡œë” ì´ˆê¸°í™”
pdf_path = "./playlist.pdf"  # ì‚¬ìš©í•˜ë ¤ëŠ” PDF íŒŒì¼ ê²½ë¡œ
loader = PyPDFLoader(pdf_path)
docs = loader.load()

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)

# ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings(),
    persist_directory=".music_recommendation_chroma_data"
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
text = st.text_area("ì˜¤ëŠ˜ì˜ ì¼ê¸°ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")

if st.button("ë¶„ì„í•˜ê¸°"):
    # ê°ì • ë¶„ì„ ìˆ˜í–‰
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    percentages = predictions * 100

    # ì„ íƒëœ ê°ì • í•„í„°ë§
    selected_emotions = [(i, emotion_labels[i][0]) for i, p in enumerate(percentages.squeeze()) if p >= 30]
    selected_emotions = sorted(selected_emotions, key=lambda x: -percentages.squeeze()[x[0]])[:2]
    if len(selected_emotions) > 1 and percentages.squeeze()[selected_emotions[0][0]] >= 60:
        selected_emotions = [selected_emotions[0]]

    st.write("ì˜ˆì¸¡ëœ ê°ì •:")
    for idx, label in selected_emotions:
        st.write(f"{label} ({percentages.squeeze()[idx]:.2f}%) {emotion_labels[idx][1]}")

    # ê°ì • ê¸°ë°˜ ì¶”ì²œ ê³¡ ìƒì„±
    emotion_keywords = [emotion_labels[idx][0].lower() for idx, _ in selected_emotions]
    query = " ".join(emotion_keywords)

    # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    with st.spinner("ì¶”ì²œ ê³¡ ê²€ìƒ‰ ì¤‘..."):
        retrieved_docs = retriever.invoke(query)

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # ChatPromptTemplate ìƒì„±
        prompt_template = ChatPromptTemplate.from_messages([
            HumanMessage(content="ë„ˆëŠ” ì‚¬ìš©ì ê°ì •ì— ë§ëŠ” ìŒì•…ì„ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì´ì•¼. ì²¨ë¶€ëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œ ê³¡ ëª©ë¡ì„ ì œì‹œí•´ì¤˜."
                                 "ë‹¤ë§Œ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ê³¡ì´ ì—†ë‹¤ë©´ ë‹µí•˜ì§€ ë§ì•„ì¤˜.\n"
                                 f"Emotion: {query}\nContext: {format_docs(retrieved_docs)}")
        ])

        # ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì— ë§ì¶° í”„ë¡¬í”„íŠ¸ ìƒì„±
        user_prompt = prompt_template.format()
        llm = ChatOpenAI(model="gpt-4o")  # LLM ëª¨ë¸ ì´ˆê¸°í™”

        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        response = llm.invoke(user_prompt)

    # ë‹µë³€ ì¶œë ¥ ë° ì„¸ì…˜ì— ì €ì¥
    with st.chat_message("assistant"):
        st.markdown(response.content)
    st.session_state.messages.append({"role": "assistant", "content": response.content})
